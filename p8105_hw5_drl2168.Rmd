---
title: "P8105 Homework 5"
author: "Derek Lamb"
date: "`r Sys.Date()`"
output: github_document
---
```{r load packages, message = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
I imported the data, noting that missing data are coded as `Unknown`. I also corrected a typo, where a homicide in Tulsa, OK was misclassified as in AL, and corrected the state representations (Wisconsin was `wi`). I then summarized the homicide data by city and state.
```{r data import}
df_homicide <- read_csv("data/homicide-data.csv", 
                        na = "Unknown") |> 
  mutate(
    state = case_when(
      city == "Tulsa" ~ "OK",
      city != "Tulsa" ~ state),
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state)
    ) |> 
  group_by(city_state) |> 
  summarize(n_homicide = n(),
            n_unsolved = sum(disposition %in% c("
                                                Closed without arrest",
                                                "Open/No arrest")))
```

Within Baltimore, MD, I calculated the proportion of unsolved cases and a confidence interval for this value.
```{r baltimore prototype}
df_homicide |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(df = broom::tidy(prop.test(x = n_unsolved, 
                                    n = n_homicide,
                                    p = 0.5,
                                    alternative = "two.sided")))
```

To improve workflow, I wrote a function to perform a z-test and construct a confidence interval under normal approximations of the data.
```{r define homicide function}
z_homicide = function(x, n){
  prop.test(x = x, n = n,
            p = 0.5, alternative = "two.sided") |> 
  broom::tidy()
}
```

Now I use this function to create a plot of the confidence interval for every city.
```{r create prop plot}
df_homicide |> 
  mutate(z_out = map2(n_unsolved, n_homicide, z_homicide)) |> 
  unnest(z_out) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = estimate, y = city_state)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(
      title = "Point and Interval Estimates of Homicide Closure",
      x = "Proportion of Unsolved Homicides",
      y = "City & State"
  )
```

# Problem 2
I imported the data using `list.files` to identify each file of interest, and then used `map` to apply `read_csv` to each file. I then tidied the data.
```{r import prob2 data, message = FALSE}
df_study <- tibble(
  path = list.files("data/prob2")
) |> 
  mutate(
         id = str_sub(path, end = 6),
         group = str_sub(path, end = 3),
         path = str_c("data/prob2/", path),
         data = map(path, read_csv)
         ) |> 
  unnest(data) |> 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "obs",
               names_prefix = "week_") |> 
  select(-path)
```

The final tidy dataframe consists of `r nrow(df_study)` observations of `r ncol(df_study)` variables: subject `id`, `group`, `week`, and the outcome `obs`.

Now I will create a spaghetti plot to compare the control and experimental arm.

```{r prob2 plot}
df_study |> 
  ggplot(aes(x = week, y = obs, 
             group = id, color = group )) + 
  geom_line() + 
  labs(
    title = "Study Outcome Over Time",
    x = "Week",
    y = "Outcome",
    color = "Experimental Group"
  )
```

The outcome in the control group appears to be relatively constant over time, while it increases over time in the experimental group. At week 1, the two groups are relatively similar, though they appear different by about week 4.

# Problem 3
Because the `map` and `map2` functions can only take 1-2 inputs, I'll create a function to generate a normal sample with only the true mean `mu` as an input.
```{r norm_samp function}
norm_samp = function(mu){
  out = rnorm(
    n = 30,
    mean = mu,
    sd = 5
  )
  return(out)
}
```

I will test this function in the $\mu = 0$ case.
```{r mu0 case}
df_sim0 <- tibble(
  mu = 0,
  iter = 1:5000
) |> 
  select(-iter) |> 
  mutate(sample = map(mu,norm_samp),
         test = map(sample, t.test),
         test = map(test, broom::tidy)) |> 
  unnest(test) |> 
  select(mu, estimate, p.value)
  
```

Since this appears to be working, I will now repeat the process, but allow the true mean to range from 0 to 6. For use in future problems, I will add the variable `reject` taking the value of 1 when `p.value` is less than 0.05, and 0 otherwise.

```{r full case}
df_sim <- expand_grid(
  mu = 0:6,
  iter = 1:5000
) |> 
  select(-iter) |> 
  mutate(sample = map(mu,norm_samp),
         test = map(sample, t.test),
         test = map(test, broom::tidy)) |> 
  unnest(test) |> 
  select(mu, estimate, p.value) |> 
  mutate(
    reject = case_when(
      p.value < 0.05 ~ 1,
      p.value >= 0.05 ~ 0)
  )
```

Now I will examine the rejection probability of these cases

```{r}
df_sim |> 
  group_by(mu) |> 
  summarize(p_reject = mean(reject)) |> 
  ggplot(aes(x = mu, y = p_reject)) + 
  geom_point()+
  labs(x = "True Mean",
       y = "Probability of rejecting H0")
```

When the true mean is 0, the probability of rejecting the null hypothesis is very close to 0.05, our prescribed $\alpha$-level. When the true mean is not equal to 0, this probability is the power, or $1-\beta$. It is low for small effects (1,2), but is almost 100% by a mean of 4. The shape of this curve is sigmoidal, and crosses 50% a bit before 2, which would probably correspond to ~2 standard errors for these distributions $2*\frac{\sigma}{\sqrt{30}} \approx 1.82$.


```{r}
df_sim |> 
  mutate(
    mu_report = case_when(
      reject == 1 ~ estimate,
      reject == 0 ~ NA
    ) 
  ) |> 
  group_by(mu) |> 
  summarize(mu_hat = mean(estimate),
            mu_bias = mean(mu_report, na.rm = TRUE)) |> 
  ggplot() + 
  geom_point(aes(x = mu, y = mu_hat, color = mu, fill = mu), 
             shape = 16, size = 2) +
  geom_point(aes(x = mu, y = mu_bias, color = mu, fill = mu), 
             shape = 18, size = 3) +
  labs(
    title = "Mean Estimates vs True Mean",
    x = "True Mean",
    y = "Etimated Mean",
    caption = "Circles: All estimates; Diamonds: Only \"significant\" estimates"
  ) + 
  theme(legend.position = "none")
  
```

The sample estimate $\hat{\mu}$ for all tests is quite close to the true mean in all cases. However, this estimate is only equal to the estimate conditioned on rejecting the null hypothesis when the probability of rejecting the null is high. As we saw in the previous figure, that is the case for $\mu \geq 4$. When the true mean is smaller than 4, the conditional estimate of $\mu$ is greater than the total estimate. This is because we are selecting only the samples with a high mean, and ignoring those with smaller means.

An exception is the $\mu = 0$ case, where the probability of rejecting the null hypothesis is equally likely to come from a negative mean or a positive one; these cancel out on average, and so the estimate remains 0.

