P8105 Homework 5
================
Derek Lamb
2023-11-12

``` r
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

I imported the data, noting that missing data are coded as `Unknown`. I
also corrected a typo, where a homicide in Tulsa, OK was misclassified
as in AL, and corrected the state representations (Wisconsin was `wi`).
I then summarized the homicide data by city and state.

``` r
df_homicide <- read_csv("data/homicide-data.csv", 
                        na = "Unknown") |> 
  mutate(
    state = case_when(
      city == "Tulsa" ~ "OK",
      city != "Tulsa" ~ state),
    state = str_to_upper(state),
    city_state = str_c(city, ", ", state)
    ) |> 
  group_by(city_state) |> 
  summarize(n_homicide = n(),
            n_unsolved = sum(disposition %in% c("
                                                Closed without arrest",
                                                "Open/No arrest")))
```

    ## Warning: One or more parsing issues, call `problems()` on your data frame for details,
    ## e.g.:
    ##   dat <- vroom(...)
    ##   problems(dat)

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Within Baltimore, MD, I calculated the proportion of unsolved cases and
a confidence interval for this value.

``` r
df_homicide |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(df = broom::tidy(prop.test(x = n_unsolved, 
                                    n = n_homicide,
                                    p = 0.5,
                                    alternative = "two.sided")))
```

    ## # A tibble: 1 × 4
    ##   city_state    n_homicide n_unsolved df$estimate $statistic $p.value $parameter
    ##   <chr>              <int>      <int>       <dbl>      <dbl>    <dbl>      <int>
    ## 1 Baltimore, MD       2827       1673       0.592       94.9 1.99e-22          1
    ## # ℹ 4 more variables: df$conf.low <dbl>, $conf.high <dbl>, $method <chr>,
    ## #   $alternative <chr>

To improve workflow, I wrote a function to perform a z-test and
construct a confidence interval under normal approximations of the data.

``` r
z_homicide = function(x, n){
  prop.test(x = x, n = n,
            p = 0.5, alternative = "two.sided") |> 
  broom::tidy()
}
```

Now I use this function to create a plot of the confidence interval for
every city.

``` r
df_homicide |> 
  mutate(z_out = map2(n_unsolved, n_homicide, z_homicide)) |> 
  unnest(z_out) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(city_state = fct_reorder(city_state, estimate)) |> 
  ggplot(aes(x = estimate, y = city_state)) + 
  geom_point() + 
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(
      title = "Point and Interval Estimates of Homicide Closure",
      x = "Proportion of Unsolved Homicides",
      y = "City & State"
  )
```

<img src="p8105_hw5_drl2168_files/figure-gfm/create prop plot-1.png" width="90%" />

# Problem 2

I imported the data using `list.files` to identify each file of
interest, and then used `map` to apply `read_csv` to each file. I then
tidied the data.

``` r
df_study <- tibble(
  path = list.files("data/prob2")
) |> 
  mutate(
         id = str_sub(path, end = 6),
         group = str_sub(path, end = 3),
         path = str_c("data/prob2/", path),
         data = map(path, read_csv)
         ) |> 
  unnest(data) |> 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "obs",
               names_prefix = "week_") |> 
  select(-path)
```

The final tidy dataframe consists of 160 observations of 4 variables:
subject `id`, `group`, `week`, and the outcome `obs`.

Now I will create a spaghetti plot to compare the control and
experimental arm.

``` r
df_study |> 
  ggplot(aes(x = week, y = obs, 
             group = id, color = group )) + 
  geom_line() + 
  labs(
    title = "Study Outcome Over Time",
    x = "Week",
    y = "Outcome",
    color = "Experimental Group"
  )
```

<img src="p8105_hw5_drl2168_files/figure-gfm/prob2 plot-1.png" width="90%" />

The outcome in the control group appears to be relatively constant over
time, while it increases over time in the experimental group. At week 1,
the two groups are relatively similar, though they appear different by
about week 4.

# Problem 3

Because the `map` and `map2` functions can only take 1-2 inputs, I’ll
create a function to generate a normal sample with only the true mean
`mu` as an input.

``` r
norm_samp = function(mu){
  out = rnorm(
    n = 30,
    mean = mu,
    sd = 5
  )
  return(out)
}
```

I will test this function in the $\mu = 0$ case.

``` r
df_sim0 <- tibble(
  mu = 0,
  iter = 1:5000
) |> 
  select(-iter) |> 
  mutate(sample = map(mu,norm_samp),
         test = map(sample, t.test),
         test = map(test, broom::tidy)) |> 
  unnest(test) |> 
  select(mu, estimate, p.value)
```

Since this appears to be working, I will now repeat the process, but
allow the true mean to range from 0 to 6.

``` r
df_sim <- expand_grid(
  mu = 0:6,
  iter = 1:5000
) |> 
  select(-iter) |> 
  mutate(sample = map(mu,norm_samp),
         test = map(sample, t.test),
         test = map(test, broom::tidy)) |> 
  unnest(test) |> 
  select(mu, estimate, p.value)
```

Now I will examine the rejection probability of these cases

``` r
df_sim |> 
  mutate(reject = case_when(
    p.value < 0.05 ~ 1,
    p.value >= 0.05 ~ 0
  )) |> 
  group_by(mu) |> 
  summarize(p_reject = mean(reject)) |> 
  ggplot(aes(x = mu, y = p_reject)) + 
  geom_point()+
  labs(x = "True Mean",
       y = "Probability of rejecting H0")
```

<img src="p8105_hw5_drl2168_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

When the true mean is 0, the probability of rejecting the null
hypothesis is very close to 0.05, our prescribed $\alpha$-level. When
the true mean is not equal to 0, this probability is the power, or
$1-\beta$. It is low for small effects (1,2), but is almost 100% by a
mean of 4. The shape of this curve is sigmoidal, and crosses 50% a bit
before 2, which would probably correspond to ~2 standard errors for
these distributions $\frac{\sigma}{\sqrt{30}} \approx 1.82$.
